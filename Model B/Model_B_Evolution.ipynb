{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4038b914",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Check for GPU\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"✅ GPU Detected: {torch.cuda.get_device_name(0)}\")\n",
    "    device = 0\n",
    "else:\n",
    "    print(\"⚠️ No GPU detected. Evolution will be extremely slow on CPU.\")\n",
    "    device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210fed30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Dataset Path Configuration \n",
    "cwd = os.getcwd()\n",
    "possible_paths = [\n",
    "    os.path.join(cwd, 'oral-diseases-1'),\n",
    "    os.path.join(cwd, 'Backend Development', 'Model B', 'oral-diseases-1'),\n",
    "    os.path.join(cwd, 'Model B', 'oral-diseases-1')\n",
    "]\n",
    "\n",
    "dataset_dir = None\n",
    "for path in possible_paths:\n",
    "    if os.path.exists(os.path.join(path, 'data.yaml')):\n",
    "        dataset_dir = path\n",
    "        break\n",
    "\n",
    "if dataset_dir is None:\n",
    "    raise FileNotFoundError(\"Could not find 'oral-diseases-1' dataset folder.\")\n",
    "\n",
    "yaml_path = os.path.join(dataset_dir, 'data.yaml')\n",
    "print(f\"✅ Using dataset at: {yaml_path}\")\n",
    "\n",
    "# Fix paths in data.yaml to be absolute\n",
    "with open(yaml_path, 'r') as f:\n",
    "    data_config = yaml.safe_load(f)\n",
    "\n",
    "data_config['train'] = os.path.join(dataset_dir, 'train', 'images')\n",
    "data_config['val'] = os.path.join(dataset_dir, 'valid', 'images')\n",
    "data_config['test'] = os.path.join(dataset_dir, 'test', 'images')\n",
    "\n",
    "with open(yaml_path, 'w') as f:\n",
    "    yaml.dump(data_config, f)\n",
    "print(\"✅ data.yaml paths updated to absolute paths.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc46efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Run Evolution ---\n",
    "# We start with your pretrained model (best.pt) to find the best hyperparameters for fine-tuning it further.\n",
    "# If you wanted to find the best parameters for training from scratch, you would use 'yolov8m.pt' here.\n",
    "model_path = 'models/best.pt'\n",
    "if not os.path.exists(model_path):\n",
    "    print(f\"⚠️ '{model_path}' not found. Falling back to 'yolov8m.pt'\")\n",
    "    model_path = 'yolov8m.pt'\n",
    "\n",
    "model = YOLO(model_path)\n",
    "\n",
    "# Determine workers based on OS (8 for Linux/HPC, 0 for Windows)\n",
    "workers = 8 if os.name != 'nt' else 0\n",
    "\n",
    "print(f\"Starting hyperparameter evolution on {model_path}...\")\n",
    "print(\"This will run multiple short training sessions to find the optimal settings.\")\n",
    "\n",
    "# Tune hyperparameters\n",
    "model.tune(\n",
    "    data=yaml_path,\n",
    "    epochs=30,          # Train for 30 epochs per iteration (short runs)\n",
    "    iterations=50,      # Run 50 genetic evolution iterations\n",
    "    optimizer='AdamW',  # Optimizer to use\n",
    "    plots=False, \n",
    "    save=False,\n",
    "    val=True,\n",
    "    device=device,\n",
    "    batch=-1,           # ✅ AutoBatch: Automatically finds the max batch size for your GPU\n",
    "    workers=workers     # ✅ Max Workers: Speeds up data loading\n",
    ")\n",
    "\n",
    "print(\"Evolution complete. The best hyperparameters are saved in the 'runs/detect/tune' folder.\")\n",
    "print(\"You can use these parameters to run a final, long training session.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c05138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 4: Final Training with Evolved Hyperparameters ---\n",
    "# ⚠️ ONLY RUN THIS AFTER THE EVOLUTION STEP ABOVE FINISHES ⚠️\n",
    "\n",
    "import glob\n",
    "\n",
    "# 1. Find the best hyperparameters file\n",
    "# The evolution saves results in runs/detect/tune/best_hyperparameters.yaml\n",
    "# We search for it dynamically just in case\n",
    "tune_dirs = glob.glob('runs/detect/tune*')\n",
    "best_hyperparams_path = None\n",
    "\n",
    "if tune_dirs:\n",
    "    # Get the most recent tune folder\n",
    "    latest_tune_dir = max(tune_dirs, key=os.path.getmtime)\n",
    "    potential_path = os.path.join(latest_tune_dir, 'best_hyperparameters.yaml')\n",
    "    if os.path.exists(potential_path):\n",
    "        best_hyperparams_path = potential_path\n",
    "\n",
    "if best_hyperparams_path:\n",
    "    print(f\"✅ Found best hyperparameters at: {best_hyperparams_path}\")\n",
    "    with open(best_hyperparams_path, 'r') as f:\n",
    "        hyperparams = yaml.safe_load(f)\n",
    "else:\n",
    "    print(\"⚠️ No hyperparameter file found. Did the evolution finish?\")\n",
    "    print(\"Using default settings for now.\")\n",
    "    hyperparams = {}\n",
    "\n",
    "# 2. Train the Final Model\n",
    "# We train for longer (e.g., 100-150 epochs) to get the best possible accuracy\n",
    "print(\"Starting Final Training with Optimized Hyperparameters...\")\n",
    "\n",
    "final_model = YOLO('models/best.pt')  # Load the best model from previous training\n",
    "\n",
    "final_model.train(\n",
    "    data=yaml_path,\n",
    "    epochs=150,         # Train for longer to maximize accuracy\n",
    "    patience=20,        # Stop if no improvement for 20 epochs\n",
    "    batch=-1,           # Use AutoBatch again\n",
    "    device=device,\n",
    "    project='oral_cancer_screening',\n",
    "    name='yolov8m_evolved_final',\n",
    "    exist_ok=True,\n",
    "    workers=8,          # Keep high workers for HPC\n",
    "    **hyperparams       # <--- This applies the evolved settings!\n",
    ")\n",
    "\n",
    "print(\"✅ Final model training complete! Your best model is in 'oral_cancer_screening/yolov8m_evolved_final/weights/best.pt'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d142af31",
   "metadata": {},
   "source": [
    "## 4. Final Training (After Evolution)\n",
    "\n",
    "Once the evolution process above finishes (which may take hours), it will save the best hyperparameters to a file.\n",
    "\n",
    "**Run the cell below** to automatically load those \"best settings\" and train your final, high-performance model."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
